# May 18, 2016

### Attendees

* Aziz Alghmadi
* Craig Mustard
* Steven Hall

### Where

CS Lab

## What Happened

More detail on the project and getting started.

## Notes:

#### Running Spark Locally

* For debugging we may want to run spark locally since it gives more debug info.

##### Steps

* Under the btbloaded makefile, you can specify the spark submit arg "deploy" to
  "local" (or just pass LOCAL=1... look at the makefile).

```bash
cd benchmark-accumulo/bdb-loader/
make run-loader LOCAL=1
```

#### Running Accumulo

* Note that we might not want to start accumulo because it might give errors.
* Might need to install sbt:
  * `sudo apt-get install sbt`

##### Steps

```bash
cd benchmark-accumulo/bdb-loader/
make all

# To start accumulo (Note the typo in the command):
cd ../../
make start-accmulo
```

#### Benchmark

Get hadoop running, pick a benchmark and put it in HDFS and use it to write some
program under spark.

Some ideas for benchmarks: PageRank or k-means. The PageRank example is
somewhere in Spark-core repo.

Benchmark Links:

* [Big Data Benchmark]
* [MLib Machine Learning]

##### Running a Benchmark Steps

```bash
cd docker-spark
make test
```

To expose the web interface:

```bash
cd docker-hadoop
make expose-yarn
make delete-expose-yarn
```

Look at the spark examples in the [documentation]

#### Cleaning Zookeeper

##### Steps

```bash
make stop
docker ps -a
docker stop
docker stop zookeeper-steven
docker rm zookeeper-steven
make start
make start-accmulo

cd docker-accumulo/
make clean
make start

# Start Accumulo Shell... Password is "accumulo"
make shell

cd docker-zookeeper
make shell
/usr/local/zookeeper/bin/zkCli.sh
ls /
```


#### TODO

* Might need to learn some Scala
* Get Added To the mailing list
* Start Creating a benchmark for spark
* Get access to the lab


[Big Data Benchmark]: https://github.com/craiig/docker-bigdata-cluster
[MLib Machine Learning]: http://spark.apache.org/docs/latest/mllib-guide.html
[documentation]: http://spark.apache.org/docs/latest/

